{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for BRATS 2019 dataset\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Dropout, Input, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import random as r\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import time\n",
    "from contextlib import redirect_stdout\n",
    "from glob import glob\n",
    "import os\n",
    "import imageio\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "#Start Execution Time\n",
    "start_time = time.time()\n",
    "\n",
    "path_in = \"...\"\n",
    "\n",
    "out_folder = 'Run_UNet++_3D/'\n",
    "path_out = \"Outputs/\" + out_folder\n",
    "\n",
    "\n",
    "if not os.path.exists(path_out):\n",
    "    os.makedirs(path_out)\n",
    "    print(\"Directory Created\")\n",
    "else:    \n",
    "    print(\"Directory Already Exists\") \n",
    "    \n",
    "files = glob(path_in + \"**/*.nii.gz\", recursive = True)\n",
    "\n",
    "print(\"No. of files : \", len(files))\n",
    "    \n",
    "\n",
    "def img_to_array(path, end):\n",
    "    \n",
    "    # get locations\n",
    "    files = glob(path+end, recursive=True)\n",
    "    \n",
    "    img_list = []\n",
    "    \n",
    "    #r.seed(42)\n",
    "    #r.shuffle(files)\n",
    "    \n",
    "    for file in files:\n",
    "        img = io.imread(file, plugin=\"simpleitk\")\n",
    "\n",
    "        # standardization\n",
    "        img = (img-img.mean())/img.std()\n",
    "        img.astype(\"float32\")\n",
    "        \n",
    "        for slice in range(60, 130):\n",
    "            img_s = img[slice,:,:]\n",
    "            \n",
    "            # resize\n",
    "            img_s = cv2.resize(img_s, (128,128))\n",
    "            \n",
    "            img_s = np.expand_dims(img_s, axis=0)\n",
    "            img_list.append(img_s)\n",
    "            \n",
    "    return np.array(img_list,np.float32)\n",
    "\n",
    "def seg_to_array(path, end, label):\n",
    "    \n",
    "    # get locations\n",
    "    files = glob(path+end, recursive=True)\n",
    "    \n",
    "    img_list = []\n",
    "    \n",
    "    #r.seed(42)\n",
    "    #r.shuffle(files)\n",
    "    \n",
    "    for file in files:\n",
    "        img = io.imread(file, plugin=\"simpleitk\")\n",
    "        \n",
    "        # all tumor\n",
    "        if label == 1:\n",
    "            img[img != 0] = 1\n",
    "        \n",
    "        # Non-enhancing Tumor\n",
    "        if label == 2:\n",
    "            img[img != 1] = 0\n",
    "        \n",
    "        # Without Edema\n",
    "        if label == 3:\n",
    "            img[img == 2] = 0\n",
    "            img[img != 0] = 1\n",
    "        \n",
    "        # Enhancing Tumor\n",
    "        if label == 4:\n",
    "            img[img != 4] = 0\n",
    "            img[img == 4] = 1\n",
    "            \n",
    "        img.astype(\"float32\")\n",
    "        \n",
    "        for slice in range(60, 130):\n",
    "            img_s = img[slice,:,:]\n",
    "            \n",
    "            # resize\n",
    "            img_s = cv2.resize(img_s, (128,128))\n",
    "            \n",
    "            img_s = np.expand_dims(img_s, axis=0)\n",
    "            img_list.append(img_s)\n",
    "            \n",
    "    return np.array(img_list,np.float32)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    print(\"y_true DCC : \", y_true)\n",
    "    print(\"y_pred DCC : \", y_pred)\n",
    "    smooth = 0.005 \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    print(\"y_true loss : \", y_true)\n",
    "    print(\"y_pred loss : \", y_pred)\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "# Function for a Convolution layer containing 2 blocks\n",
    "def conv2d_block( input_tensor, n_filters, kernel_size = (3,3), name=\"contraction\"):\n",
    "  \"Add 2 conv layer\"\n",
    "  x = Conv2D(filters=n_filters, kernel_size=kernel_size, kernel_initializer='he_normal', \n",
    "             padding='same',activation=\"relu\", name=name+'_1')(input_tensor)\n",
    "  \n",
    "  x = Conv2D(filters=n_filters, kernel_size=kernel_size, kernel_initializer='he_normal', \n",
    "             padding='same',activation=\"relu\",name=name+'_2')(x)\n",
    "  return x\n",
    "\n",
    "t1 = img_to_array(path=path, end=\"**/*t1.nii.gz\")\n",
    "t1ce = img_to_array(path=path, end=\"**/*t1ce.nii.gz\")\n",
    "t2 = img_to_array(path=path, end=\"**/*t2.nii.gz\")\n",
    "flair = img_to_array(path=path, end=\"**/*flair.nii.gz\")\n",
    "\n",
    "seg = seg_to_array(path=path, end=\"**/*seg.nii.gz\", label=1)\n",
    "\n",
    "print(\"t1 shape : \", t1.shape) \n",
    "print(\"t1ce shape : \", t1ce.shape) \n",
    "print(\"t2 shape : \", t2.shape) \n",
    "print(\"flair shape : \", flair.shape)\n",
    "\n",
    "print(\"seg shape : \", seg.shape)\n",
    "\n",
    "X = np.concatenate((t1, t1ce, t2, flair), axis=1)\n",
    "print(\"X_train shape : \", X.shape)\n",
    "print(\"X_train datatype : \", X.dtype)\n",
    "\n",
    "# Training Test division\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,seg,test_size=0.2)\n",
    "del X\n",
    "del seg\n",
    "gc.collect()\n",
    "print(\"X_train shape : \", X_train.shape)\n",
    "print(\"X_train datatype : \", X_train.dtype)\n",
    "print(\"X_train, X_test Done.\")\n",
    "\n",
    "# Training is divided into training and validation\n",
    "X_train1,X_val,Y_train1,Y_val = train_test_split(X_train,Y_train,test_size=0.2)\n",
    "\n",
    "file0 = open(path_out + 'Data_division.txt','w')\n",
    "file0.write(\"Training Data: \" + str(Y_train.shape)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Test Data: \" + str(Y_test.shape)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Final Training Data: \" + str(Y_train1.shape)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Validation Data: \" + str(Y_val.shape)) \n",
    "file0.close()\n",
    "\n",
    "del X_train\n",
    "del Y_train\n",
    "gc.collect()\n",
    "\n",
    "print(\"X_train1, X_test1 Done.\")\n",
    "    \n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "def unetpp():\n",
    "    \n",
    "    inputs = Input((4, 128 , 128))\n",
    "    # ------------------------------------------------------------------------\n",
    "    # X(0,0)\n",
    "    d1 = conv2d_block(inputs, 64, name=\"contraction_1\") # None, 64, 128, 128\n",
    "    #print(\"d1: \", d1) \n",
    "    p1 = MaxPooling2D( pool_size=(2,2), strides=(2,2), padding='same')(d1)\n",
    "    p1 = BatchNormalization(momentum=0.8)(p1)\n",
    "    p1 = Dropout(0.1)(p1) # (None, 64, 64, 64)\n",
    "    #print(\"p1:\", p1)\n",
    "\n",
    "    # X(1,0)\n",
    "    d2 = conv2d_block( p1, 128, name=\"contraction_2_1\" )\n",
    "    #print(\"d2: \", d2)\n",
    "    p2 = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(d2)\n",
    "    p2 = BatchNormalization(momentum=0.8)(p2)\n",
    "    p2 = Dropout(0.1)(p2) # (None, 128, 32, 32)\n",
    "    #print(\"p2:\", p2)\n",
    "\n",
    "    # X(2,0)\n",
    "    d3 = conv2d_block( p2, 256, name=\"contraction_3_1\")\n",
    "    #print(\"d3: \", d3)\n",
    "    p3 = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(d3)\n",
    "    p3 = BatchNormalization(momentum=0.8)(p3)\n",
    "    p3 = Dropout(0.1)(p3) # (None, 256, 16, 16)\n",
    "    #print(\"p3:\", p3)\n",
    "\n",
    "    # X(3,0)\n",
    "    d4 = conv2d_block(p3,512, name=\"contraction_4_1\")\n",
    "    #print(\"d4: \", d4)\n",
    "    p4 = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(d4)\n",
    "    p4 = BatchNormalization(momentum=0.8)(p4)\n",
    "    p4 = Dropout(0.1)(p4)\n",
    "    #print(\"p4:\", p4) # (None, 512, 8, 8)\n",
    "\n",
    "    # X(4,0)\n",
    "    d5 = conv2d_block(p4,512, name=\"contraction_5_1\")\n",
    "    #print(\"d5:\", d5) # (None, 512, 8, 8)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # ---------------- UNet++ L1 ---------------\n",
    "    # X(0,1)\n",
    "    du_01 = Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = 'same')(d2)\n",
    "    #print(\"du_01: \", du_01) # 64, 128, 128\n",
    "    du_01 = concatenate([du_01,d1], axis=1) # d1 = None, 64, 128, 128\n",
    "    #print(\"du_01 concat: \", du_01) # None, 64, 128, 256\n",
    "    du_01 = Dropout(0.1)(du_01)\n",
    "    c_01 = conv2d_block(du_01, 64, name=\"expansion_01\")\n",
    "    #print(\"c_01: \", c_01)\n",
    "\n",
    "    # ---------------- UNet++ L2 ---------------\n",
    "    # X(1,1)\n",
    "    du_11 = Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = 'same')(d3)\n",
    "    du_11 = concatenate([du_11,d2], axis=1)\n",
    "    du_11 = Dropout(0.1)(du_11)\n",
    "    c_11 = conv2d_block(du_11, 128, name=\"expansion_11\")\n",
    "    #print(\"c_11: \", c_11)\n",
    "\n",
    "    # X(0,2)\n",
    "    du_02 = Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = 'same')(c_11)\n",
    "    du_02 = concatenate([du_02,du_01,d1], axis=1)\n",
    "    du_02 = Dropout(0.1)(du_02)\n",
    "    c_02 = conv2d_block(du_02, 64, name=\"expansion_02\")\n",
    "    #print(\"c_02: \", c_02)\n",
    "\n",
    "    # ---------------- UNet++ L3 ---------------\n",
    "    # X(2,1)\n",
    "    du_21 = Conv2DTranspose(256, (3, 3), strides = (2, 2), padding = 'same')(d4)\n",
    "    du_21 = concatenate([du_21,d3], axis=1)\n",
    "    du_21 = Dropout(0.1)(du_21)\n",
    "    c_21 = conv2d_block(du_21, 256, name=\"expansion_21\")\n",
    "    #print(\"c_21: \", c_21)\n",
    "\n",
    "    # X(1,2)\n",
    "    du_12 = Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = 'same')(c_21)\n",
    "    du_12 = concatenate([du_12,c_11,d2], axis=1)\n",
    "    du_12 = Dropout(0.1)(du_12)\n",
    "    c_12 = conv2d_block(du_12, 128, name=\"expansion_12\")\n",
    "    #print(\"c_12: \", c_12)\n",
    "\n",
    "    # X(0,3)\n",
    "    du_03 = Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = 'same')(c_12)\n",
    "    du_03 = concatenate([du_03,c_02,c_01,d1], axis=1)\n",
    "    du_03 = Dropout(0.1)(du_03)\n",
    "    c_03 = conv2d_block(du_03, 64, name=\"expansion_03\")\n",
    "    #print(\"c_03: \", c_03)\n",
    "\n",
    "    # ---------------- UNet++ L4 ---------------\n",
    "    # X(3,1)\n",
    "    u_31 = Conv2DTranspose(512, (3, 3), strides = (2, 2), padding = 'same')(d5)\n",
    "    u_31 = concatenate([u_31,d4], axis=1)\n",
    "    u_31 = Dropout(0.1)(u_31)\n",
    "    c_31 = conv2d_block(u_31, 256, name=\"expansion_1\")\n",
    "    #print(\"c_31: \", c_31)\n",
    "\n",
    "    # X(2,2)\n",
    "    u_22 = Conv2DTranspose(256, (3, 3), strides = (2, 2), padding = 'same')(c_31)\n",
    "    u_22 = concatenate([u_22,c_21,d3], axis=1)\n",
    "    u_22 = Dropout(0.1)(u_22)\n",
    "    c_22 = conv2d_block(u_22, 256, name=\"expansion_2\")\n",
    "    #print(\"c_22: \", c_22)\n",
    "\n",
    "    # X(1,3)\n",
    "    u_13 = Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = 'same')(c_22)\n",
    "    u_13 = concatenate([u_13,c_12,c_11,d2], axis=1)\n",
    "    u_13 = Dropout(0.1)(u_13)\n",
    "    c_13 = conv2d_block(u_13, 128, name=\"expansion_3\")\n",
    "    #print(\"c_13: \", c_13)\n",
    "\n",
    "    # X(0,4)\n",
    "    u_04 = Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = 'same')(c_13)\n",
    "    u_04 = concatenate([u_04,c_03,c_02,c_01,d1], axis=1)\n",
    "    u_04 = Dropout(0.1)(u_04)\n",
    "    c_04 = conv2d_block(u_04, 64, name=\"expansion_4\")\n",
    "    #print(\"c_04: \", c_04)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Final concatenation\n",
    "    c_L = concatenate([c_01,c_02,c_03,c_04], axis=1)\n",
    "    #print(\"c_L: \", c_L)\n",
    "\n",
    "    out = Conv2D(1, (1,1), name=\"output\", activation='sigmoid')(c_L)\n",
    "    \n",
    "    #print(\"inputs: \", inputs)\n",
    "    #print(\"out: \", out)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[out])\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-4), loss=dice_coef_loss, metrics=['acc', dice_coef])\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = unetpp()\n",
    "\n",
    "with open(path_out + 'modelsummary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()\n",
    "\n",
    "model.save_weights(path_out + \"model_weights.h5\")\n",
    "\n",
    "hist = model.fit(X_train1, Y_train1, batch_size=8, epochs=50, \n",
    "                validation_data=(X_val,Y_val), verbose=1)\n",
    "print(\"Model fit Done.\")\n",
    "\n",
    "# Calculating and drawing training plots\n",
    "acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "dice = hist.history['dice_coef']\n",
    "val_dice = hist.history['val_dice_coef']\n",
    "\n",
    "avg_acc = np.mean(acc) * 100\n",
    "avg_val_acc = np.mean(val_acc) * 100\n",
    "avg_loss = np.mean(loss) * 100\n",
    "avg_val_loss = np.mean(val_loss) * 100\n",
    "avg_dcc = np.mean(dice) * 100\n",
    "avg_val_dcc = np.mean(val_dice) * 100\n",
    "file0 = open(path_out + 'avg_eval.txt','w')#append mode \n",
    "file0.write(\"Average Training Accuracy: \" + str(avg_acc)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Average Validation Accuracy: \" + str(avg_val_acc)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Average Training DCC: \" + str(avg_dcc)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Average Validation DCC: \" + str(avg_val_dcc)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Average Training Loss: \" + str(avg_loss)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Average Validation Loss: \" + str(avg_val_loss)) \n",
    "file0.close()\n",
    "\n",
    "np.savetxt(path_out + 'train_acc.txt', acc, delimiter=\"\\n\") \n",
    "np.savetxt(path_out + 'val_acc.txt', val_acc, delimiter=\"\\n\") \n",
    "np.savetxt(path_out + 'train_loss.txt', loss, delimiter=\"\\n\") \n",
    "np.savetxt(path_out + 'val_loss.txt', val_loss, delimiter=\"\\n\") \n",
    "np.savetxt(path_out + 'dcc.txt', dice, delimiter=\"\\n\") \n",
    "np.savetxt(path_out + 'val_dcc.txt', val_dice, delimiter=\"\\n\") \n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 4))\n",
    "t = f.suptitle('Unet++ Performance in Segmenting Tumors', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "epoch_list = hist.epoch\n",
    "\n",
    "ax1.plot(epoch_list, hist.history['acc'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, hist.history['val_acc'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(0, epoch_list[-1], 5))\n",
    "ax1.set_ylabel('Accuracy Value');ax1.set_xlabel('Epoch');ax1.set_title('Accuracy')\n",
    "ax1.legend(loc=\"best\");ax1.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "ax2.plot(epoch_list, hist.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, hist.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(0, epoch_list[-1], 5))\n",
    "ax2.set_ylabel('Loss Value');ax2.set_xlabel('Epoch');ax2.set_title('Loss')\n",
    "ax2.legend(loc=\"best\");ax2.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "ax3.plot(epoch_list, hist.history['dice_coef'], label='Train DCC')\n",
    "ax3.plot(epoch_list, hist.history['val_dice_coef'], label='Validation DCC')\n",
    "ax3.set_xticks(np.arange(0, epoch_list[-1], 5))\n",
    "ax3.set_ylabel('DCC Value');ax3.set_xlabel('Epoch');ax3.set_title('DCC')\n",
    "ax3.legend(loc=\"best\");ax3.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.savefig(path_out + 'Performance_Plot.jpg')\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "np.save(path_out + 'test_tumor.npy', Y_test)\n",
    "np.save(path_out + 'predicted_tumor.npy', predicted)\n",
    "\n",
    "test_eval = model.evaluate(X_test, Y_test, verbose=0)\n",
    "test_loss = test_eval[0] * 100\n",
    "test_acc = test_eval[1] * 100\n",
    "test_dcc = test_eval[2] * 100\n",
    "print('Test accuracy: ', test_acc)\n",
    "print('Test loss: ', test_loss)\n",
    "print('Test dcc: ', test_dcc)\n",
    "\n",
    "file_t = open(path_out + 'test_eval.txt','w')#append mode \n",
    "file_t.write(\"Test Accuracy: \" + str(test_acc)) \n",
    "file_t.write(\"\\n\")\n",
    "file_t.write(\"Test Loss: \" + str(test_loss)) \n",
    "file_t.write(\"\\n\")\n",
    "file_t.write(\"Test DCC: \" + str(test_dcc)) \n",
    "file_t.close() \n",
    "\n",
    "pred_path = path_out + 'Prediction/'\n",
    "\n",
    "if not os.path.exists(pred_path):\n",
    "    os.makedirs(pred_path)\n",
    "    print(\"Directory Created\")\n",
    "else:    \n",
    "    print(\"Directory Already Exists\") \n",
    "\n",
    "i = 0\n",
    "for idx in range(0,X_test.shape[0]):\n",
    "    i = i + 1\n",
    "    imageio.imwrite(pred_path + str(i) + '_MRI.jpg', X_test[idx][0])\n",
    "    imageio.imwrite(pred_path + str(i) + '_Mask.jpg', Y_test[idx][0])\n",
    "    imageio.imwrite(pred_path + str(i) + '_PredMask.jpg', predicted[idx][0])\n",
    "    \n",
    "\n",
    "# Shows predicted outputs by choosing 10 Random images\n",
    "plt.figure(figsize=(8,30))\n",
    "i=1;total=10\n",
    "t = 0\n",
    "for idx in np.random.randint(0,high=X_test.shape[0],size=total):\n",
    "    t = t + 1\n",
    "    plt.subplot(total,3,i);i+=1\n",
    "    plt.imshow(X_test[idx][0], cmap=\"inferno\")\n",
    "    plt.title(\"MRI\");plt.axis('off')\n",
    "    \n",
    "    plt.subplot(total,3,i);i+=1\n",
    "    plt.imshow(Y_test[idx][0], cmap=\"inferno\")\n",
    "    plt.title(\"Ground Truth\");plt.axis('off')\n",
    "    \n",
    "    plt.subplot(total,3,i);i+=1\n",
    "    plt.imshow(predicted[idx][0], cmap=\"inferno\")\n",
    "    plt.title(\"Predicted Tumor\");plt.axis('off')\n",
    "\n",
    "plt.savefig(path_out + 'Sample_Output_Plot.jpg')\n",
    "\n",
    "\n",
    "#End Time\n",
    "print(\"--- Time : %s seconds ---\" % (time.time() - start_time))\n",
    "f_time = open(path_out + 'Time_Info.txt', 'w')\n",
    "print(\"%s %f\" % (\"Execution Time: \", time.time() - start_time), file=f_time)\n",
    "f_time.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
